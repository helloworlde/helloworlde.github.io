<!doctype html><html lang=en><head><meta name=viewport content="width=device-width,initial-scale=1"><title>将重新训练的 Yolov8 模型编译为 Hailo 8 和 Hailo 8L 支持的模型</title>
<meta charset=utf-8><meta name=description content="Ladder@在 Ubuntu 22 将自行训练的 Yolov8n 模型编译为 Hailo8/Hailo8L 支持的 hef 格式的模型，在 Hailo8 上使用自行训练的模型进行对象检测
1. 环境准备

anaconda

参考 Installing on Linux 安装
2. 训练 yolo 模型
2.1 安装依赖

创建环境

单独创建 ultralytics 的环境，用于训练和导出 yolo 模型为其他格式
conda create -n ultralytics python=3.10

激活环境

conda activate ultralytics

安装 ultralytics

pip install ultralytics
等待安装完成后就可以使用 yolo 命令训练和导出模型了
2.2 训练模型
训练模型可以参考 roboflow 的博客 How to Train YOLOv8 Object Detection on a Custom Dataset；为了测试验证流程，先使用官方提供的 yolov8n 当作训练的模型，可以从 https://github.com/ultralytics/assets/releases/ 下载，以 yolov8n 为例，下载地址为 https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt
wget https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt
2.3 将模型导出为 onnx 格式
yolo export model=./yolov8n.pt imgsz=640 format=onnx opset=11
imgsz=640 指定输入图片的尺寸（图像大小），即 640x640。这个尺寸是模型在推理时处理图像的分辨率"><meta name=author content="HelloWood"><link rel=canonical href=https://blog.hellowood.dev/posts/%E5%B0%86%E9%87%8D%E6%96%B0%E8%AE%AD%E7%BB%83%E7%9A%84-yolov8-%E6%A8%A1%E5%9E%8B%E7%BC%96%E8%AF%91%E4%B8%BA-hailo-8-%E5%92%8C-hailo-8l-%E6%94%AF%E6%8C%81%E7%9A%84%E6%A8%A1%E5%9E%8B/><meta name=google-site-verification content="G-3MSGPYTHPZ"><link rel=alternate type=application/rss+xml href=https://blog.hellowood.dev//index.xml title=HelloWood><script async src="https://www.googletagmanager.com/gtag/js?id=G-3MSGPYTHPZ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-3MSGPYTHPZ")}</script><script async defer data-website-id=7a9034de-0ce7-4756-942a-829e6cd22301 src=https://umami.hellowood.dev/script.js></script><script defer data-cf-beacon='{"token": "b2e481d136dc428c8c96c8673e2a04cf"}' src=https://static.cloudflareinsights.com/beacon.min.js></script><meta property="og:url" content="https://blog.hellowood.dev/posts/%E5%B0%86%E9%87%8D%E6%96%B0%E8%AE%AD%E7%BB%83%E7%9A%84-yolov8-%E6%A8%A1%E5%9E%8B%E7%BC%96%E8%AF%91%E4%B8%BA-hailo-8-%E5%92%8C-hailo-8l-%E6%94%AF%E6%8C%81%E7%9A%84%E6%A8%A1%E5%9E%8B/"><meta property="og:site_name" content="HelloWood"><meta property="og:title" content="将重新训练的 Yolov8 模型编译为 Hailo 8 和 Hailo 8L 支持的模型"><meta property="og:description" content="在 Ubuntu 22 将自行训练的 Yolov8n 模型编译为 Hailo8/Hailo8L 支持的 hef 格式的模型，在 Hailo8 上使用自行训练的模型进行对象检测
1. 环境准备 anaconda 参考 Installing on Linux 安装
2. 训练 yolo 模型 2.1 安装依赖 创建环境 单独创建 ultralytics 的环境，用于训练和导出 yolo 模型为其他格式
conda create -n ultralytics python=3.10 激活环境 conda activate ultralytics 安装 ultralytics pip install ultralytics 等待安装完成后就可以使用 yolo 命令训练和导出模型了
2.2 训练模型 训练模型可以参考 roboflow 的博客 How to Train YOLOv8 Object Detection on a Custom Dataset；为了测试验证流程，先使用官方提供的 yolov8n 当作训练的模型，可以从 https://github.com/ultralytics/assets/releases/ 下载，以 yolov8n 为例，下载地址为 https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt
wget https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt 2.3 将模型导出为 onnx 格式 yolo export model=./yolov8n.pt imgsz=640 format=onnx opset=11 imgsz=640 指定输入图片的尺寸（图像大小），即 640x640。这个尺寸是模型在推理时处理图像的分辨率"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-08T21:32:21+08:00"><meta property="article:modified_time" content="2024-09-08T21:32:21+08:00"><meta property="article:tag" content="Ubuntu"><meta property="article:tag" content="Hailo"><meta property="article:tag" content="TPU"><meta property="og:see_also" content="https://blog.hellowood.dev/posts/ubuntu22-%E5%AE%89%E8%A3%85%E5%88%9D%E5%A7%8B%E5%8C%96-hailo-8%E7%B3%BB%E5%88%97-tpu-%E5%8A%A0%E9%80%9F%E5%99%A8/"><meta property="og:see_also" content="https://blog.hellowood.dev/posts/ubuntu-22-%E5%9C%A8-docker-%E5%AE%B9%E5%99%A8%E4%B8%AD%E4%BD%BF%E7%94%A8-nvidia-%E6%98%BE%E5%8D%A1/"><meta property="og:see_also" content="https://blog.hellowood.dev/posts/ubuntu-22-%E8%BF%90%E8%A1%8C-google-coral-tpu-%E7%A4%BA%E4%BE%8B/"><meta property="og:see_also" content="https://blog.hellowood.dev/posts/ubuntu-22-%E5%AE%89%E8%A3%85-google-coral-tpu-nvme-%E9%A9%B1%E5%8A%A8/"><meta property="og:see_also" content="https://blog.hellowood.dev/posts/ubuntu-22-%E5%AE%89%E8%A3%85-nvidia-%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8%E5%92%8C-cuda/"><meta property="og:see_also" content="https://blog.hellowood.dev/posts/ubuntu-22-%E5%AE%89%E8%A3%85-intel-n100-%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8/"><meta property="og:see_also" content="https://blog.hellowood.dev/posts/frigate-%E4%BD%BF%E7%94%A8-hailo-8-%E6%88%96-hailo-8l-%E8%BF%9B%E8%A1%8C%E5%AF%B9%E8%B1%A1%E8%AF%86%E5%88%AB/"><meta property="og:see_also" content="https://blog.hellowood.dev/posts/ubuntu22-%E5%AE%89%E8%A3%85%E5%88%9D%E5%A7%8B%E5%8C%96-hailo-8%E7%B3%BB%E5%88%97-tpu-%E5%8A%A0%E9%80%9F%E5%99%A8/"><meta property="og:see_also" content="https://blog.hellowood.dev/posts/frigate-%E4%BD%BF%E7%94%A8-hailo-8-%E6%88%96-hailo-8l-%E8%BF%9B%E8%A1%8C%E5%AF%B9%E8%B1%A1%E8%AF%86%E5%88%AB/"><meta property="og:see_also" content="https://blog.hellowood.dev/posts/ubuntu22-%E5%AE%89%E8%A3%85%E5%88%9D%E5%A7%8B%E5%8C%96-hailo-8%E7%B3%BB%E5%88%97-tpu-%E5%8A%A0%E9%80%9F%E5%99%A8/"><meta name=twitter:card content="summary"><meta name=twitter:title content="将重新训练的 Yolov8 模型编译为 Hailo 8 和 Hailo 8L 支持的模型"><meta name=twitter:description content="在 Ubuntu 22 将自行训练的 Yolov8n 模型编译为 Hailo8/Hailo8L 支持的 hef 格式的模型，在 Hailo8 上使用自行训练的模型进行对象检测
1. 环境准备 anaconda 参考 Installing on Linux 安装
2. 训练 yolo 模型 2.1 安装依赖 创建环境 单独创建 ultralytics 的环境，用于训练和导出 yolo 模型为其他格式
conda create -n ultralytics python=3.10 激活环境 conda activate ultralytics 安装 ultralytics pip install ultralytics 等待安装完成后就可以使用 yolo 命令训练和导出模型了
2.2 训练模型 训练模型可以参考 roboflow 的博客 How to Train YOLOv8 Object Detection on a Custom Dataset；为了测试验证流程，先使用官方提供的 yolov8n 当作训练的模型，可以从 https://github.com/ultralytics/assets/releases/ 下载，以 yolov8n 为例，下载地址为 https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt
wget https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt 2.3 将模型导出为 onnx 格式 yolo export model=./yolov8n.pt imgsz=640 format=onnx opset=11 imgsz=640 指定输入图片的尺寸（图像大小），即 640x640。这个尺寸是模型在推理时处理图像的分辨率"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.hellowood.dev/posts/"},{"@type":"ListItem","position":2,"name":"将重新训练的 Yolov8 模型编译为 Hailo 8 和 Hailo 8L 支持的模型","item":"https://blog.hellowood.dev/posts/%E5%B0%86%E9%87%8D%E6%96%B0%E8%AE%AD%E7%BB%83%E7%9A%84-yolov8-%E6%A8%A1%E5%9E%8B%E7%BC%96%E8%AF%91%E4%B8%BA-hailo-8-%E5%92%8C-hailo-8l-%E6%94%AF%E6%8C%81%E7%9A%84%E6%A8%A1%E5%9E%8B/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"将重新训练的 Yolov8 模型编译为 Hailo 8 和 Hailo 8L 支持的模型","name":"将重新训练的 Yolov8 模型编译为 Hailo 8 和 Hailo 8L 支持的模型","description":"在 Ubuntu 22 将自行训练的 Yolov8n 模型编译为 Hailo8/Hailo8L 支持的 hef 格式的模型，在 Hailo8 上使用自行训练的模型进行对象检测\n1. 环境准备 anaconda 参考 Installing on Linux 安装\n2. 训练 yolo 模型 2.1 安装依赖 创建环境 单独创建 ultralytics 的环境，用于训练和导出 yolo 模型为其他格式\nconda create -n ultralytics python=3.10 激活环境 conda activate ultralytics 安装 ultralytics pip install ultralytics 等待安装完成后就可以使用 yolo 命令训练和导出模型了\n2.2 训练模型 训练模型可以参考 roboflow 的博客 How to Train YOLOv8 Object Detection on a Custom Dataset；为了测试验证流程，先使用官方提供的 yolov8n 当作训练的模型，可以从 https://github.com/ultralytics/assets/releases/ 下载，以 yolov8n 为例，下载地址为 https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt\nwget https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt 2.3 将模型导出为 onnx 格式 yolo export model=./yolov8n.pt imgsz=640 format=onnx opset=11 imgsz=640 指定输入图片的尺寸（图像大小），即 640x640。这个尺寸是模型在推理时处理图像的分辨率\n","keywords":["Ubuntu","Hailo","TPU"],"articleBody":"在 Ubuntu 22 将自行训练的 Yolov8n 模型编译为 Hailo8/Hailo8L 支持的 hef 格式的模型，在 Hailo8 上使用自行训练的模型进行对象检测\n1. 环境准备 anaconda 参考 Installing on Linux 安装\n2. 训练 yolo 模型 2.1 安装依赖 创建环境 单独创建 ultralytics 的环境，用于训练和导出 yolo 模型为其他格式\nconda create -n ultralytics python=3.10 激活环境 conda activate ultralytics 安装 ultralytics pip install ultralytics 等待安装完成后就可以使用 yolo 命令训练和导出模型了\n2.2 训练模型 训练模型可以参考 roboflow 的博客 How to Train YOLOv8 Object Detection on a Custom Dataset；为了测试验证流程，先使用官方提供的 yolov8n 当作训练的模型，可以从 https://github.com/ultralytics/assets/releases/ 下载，以 yolov8n 为例，下载地址为 https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt\nwget https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt 2.3 将模型导出为 onnx 格式 yolo export model=./yolov8n.pt imgsz=640 format=onnx opset=11 imgsz=640 指定输入图片的尺寸（图像大小），即 640x640。这个尺寸是模型在推理时处理图像的分辨率\nformat=onnx 指定导出的模型格式为 ONNX（Open Neural Network Exchange）。ONNX 是一种支持在不同深度学习框架间转换和共享模型的开放格式\nopset=11 指定 ONNX 的操作集版本为 11\n3. 编译为 Hailo 模型 3.1 准备环境 3.1.1 创建环境 创建环境 单独创建一个 hailo 的环境，避免和 ultralytics 依赖冲突\nconda create -n hailo python=3.10 激活环境 conda activate hailo 3.1.2 安装 hailo_model_zoo 安装 hailo_model_zoo 从 Developer Zone 下载并安装 hailo_model_zoo\npip install hailo_model_zoo-2.12.0-py3-none-any.whl 3.1.3 准备 Coco 数据集 安装 Coco 数据集，用于评估、优化和编译模型；因为是通过 conda 安装的，代码所在路径是 conda env 路径 + 环境名称 + 包路径，即 ~/anaconda3/envs/hailo/lib/python3.10/site-packages/hailo_model_zoo\n准备 val 数据 python ~/anaconda3/envs/hailo/lib/python3.10/site-packages/hailo_model_zoo/datasets/create_coco_tfrecord.py val2017 将会下载 coco 的 val 数据到 ~/.hailomz 路径下；val 数据用于验证模型\n2024-09-08 16:53:52.437311: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2024-09-08 16:53:52.438390: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used. 2024-09-08 16:53:52.458864: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used. 2024-09-08 16:53:52.459101: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations. To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-09-08 16:53:52.747289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT 48 / 5000 images have no annotations val2017 #5000: /home/ubuntu/.hailomz/data/coco/val2017/000000365098.jpg: 100%|████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:09\u003c00:00, 526.82it/s] 准备 calib 数据 python ~/anaconda3/envs/hailo/lib/python3.10/site-packages/hailo_model_zoo/datasets/create_coco_tfrecord.py calib2017 会下载名为 train2017.zip 大约 18G 的 Coco 数据集解压并处理，路径是 ～/.hailomz；calib 数据集用于模型的量化校准\n2024-09-08 16:54:19.758265: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`. 2024-09-08 16:54:19.759360: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used. 2024-09-08 16:54:19.779913: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used. 2024-09-08 16:54:19.780151: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations. To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags. 2024-09-08 16:54:20.070489: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT 1021 / 118287 images have no annotations calib2017 #8192: /home/ubuntu/.hailomz/data/coco/train2017/000000171472.jpg: 100%|████████████████████████████████████████████████████████████████████████████████████| 8192/8192 [00:15\u003c00:00, 529.18it/s] Done converting 8192 images 3.1.4 安装 hailo_dataflow_compiler 安装 hailo_dataflow_compiler 依赖 hailo_dataflow_compiler 用于将其他模型编译为 Hailo 支持的模型；在安装 hailo_dataflow_compiler 之前，需要先安装 pygraphviz，如果通过 hailo_dataflow_compiler 安装可能会提示 fatal error: graphviz/cgraph.h: 没有那个文件或目录，改成用 conda 安装即可\nconda install pygraphviz 从 Developer Zone 下载并安装 hailo_dataflow_compiler\npip install hailo_dataflow_compiler-3.28.0-py3-none-linux_x86_64.whl 3.2 编译模型 Hailo 不同硬件的模型不通用，所以需要在编译时通过 --hw-arch 指定硬件类型，以 Hailo8 为例，需要指定 --hw-arch hailo8；如果是 Hailo8L 则指定 --hw-arch hailo8l\n3.2.1 解析模型 parse 用于将模型从各种框架解析为 HAR 格式（Hailo Archive）的步骤，HAR 是一个 tar.gz 存档文件，其中包含部署到 Hailo 运行时的图结构和权重的表示形式\nhailomz parse --hw-arch hailo8 --ckpt ./best.onnx yolov8n Start run for network yolov8n ... Initializing the runner... [info] Translation started on ONNX model yolov8n [info] Restored ONNX model yolov8n (completion time: 00:00:00.04) [info] Extracted ONNXRuntime meta-data for Hailo model (completion time: 00:00:00.26) [info] NMS structure of yolov8 (or equivalent architecture) was detected. [info] In order to use HailoRT post-processing capabilities, these end node names should be used: /model.22/cv2.0/cv2.0.2/Conv /model.22/cv3.0/cv3.0.2/Conv /model.22/cv2.1/cv2.1.2/Conv /model.22/cv3.1/cv3.1.2/Conv /model.22/cv2.2/cv2.2.2/Conv /model.22/cv3.2/cv3.2.2/Conv. [info] Start nodes mapped from original model: 'images': 'yolov8n/input_layer1'. [info] End nodes mapped from original model: '/model.22/cv2.0/cv2.0.2/Conv', '/model.22/cv3.0/cv3.0.2/Conv', '/model.22/cv2.1/cv2.1.2/Conv', '/model.22/cv3.1/cv3.1.2/Conv', '/model.22/cv2.2/cv2.2.2/Conv', '/model.22/cv3.2/cv3.2.2/Conv'. [info] Translation completed on ONNX model yolov8n (completion time: 00:00:00.67) [info] Saved HAR to: /home/ubuntu/workspace/hailo/custom-train/data/model/yolov8n.har 3.2.2 量化模型 optimize 输入是 Hailo 模型状态下的 HAR 文件（优化前；具有本机权重），输出将是具有量化权重的量化 HAR 文件\nhailomz optimize --hw-arch hailo8 --har ./yolov8n.har yolov8n 输出如下；如果这个过程提示 FileNotFoundError: no alls found for requested hw_arch，参考后面问题部分处理\nStart run for network yolov8n ... Initializing the hailo8 runner... Preparing calibration data... [info] Loading model script commands to yolov8n from /home/ubuntu/anaconda3/envs/hailo/lib/python3.10/site-packages/hailo_model_zoo/cfg/alls/hailo8/base/yolov8n.alls [info] Starting Model Optimization [warning] Reducing optimization level to 0 (the accuracy won't be optimized and compression won't be used) because there's no available GPU [warning] Running model optimization with zero level of optimization is not recommended for production use and might lead to suboptimal accuracy results [info] Model received quantization params from the hn [info] Starting Mixed Precision [info] Mixed Precision is done (completion time is 00:00:00.26) [info] Layer Norm Decomposition skipped [info] Starting Stats Collector [info] Using dataset with 64 entries for calibration Calibration: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 [00:15\u003c00:00, 4.25entries/s] [info] Stats Collector is done (completion time is 00:00:15.89) [info] Starting Fix zp_comp Encoding [info] Fix zp_comp Encoding is done (completion time is 00:00:00.00) [info] matmul_equalization skipped [info] Finetune encoding skipped [info] Bias Correction skipped [info] Adaround skipped [info] Fine Tune skipped [info] Layer Noise Analysis skipped [info] Model Optimization is done [info] Saved HAR to: /home/ubuntu/workspace/hailo/custom-train/data/model/yolov8n.har 3.2.3 编译模型 compile 将模型转换为HEF可执行格式\nhailomz compile yolov8n --hw-arch hailo8 --har ./yolov8n.har 输出如下：\nStart run for network yolov8n ... Initializing the hailo8 runner... [info] Loading model script commands to yolov8n from /home/ubuntu/anaconda3/envs/hailo/lib/python3.10/site-packages/hailo_model_zoo/cfg/alls/hailo8/base/yolov8n.alls [info] To achieve optimal performance, set the compiler_optimization_level to \"max\" by adding performance_param(compiler_optimization_level=max) to the model script. Note that this may increase compilation time. [info] Adding an output layer after conv41 [info] Adding an output layer after conv42 [info] Adding an output layer after conv52 [info] Adding an output layer after conv53 [info] Adding an output layer after conv62 [info] Adding an output layer after conv63 [info] Loading network parameters [warning] Output order different size [info] Starting Hailo allocation and compilation flow [info] Using Single-context flow [info] Resources optimization guidelines: Strategy -\u003e GREEDY Objective -\u003e MAX_FPS [info] Resources optimization params: max_control_utilization=75%, max_compute_utilization=75%, max_compute_16bit_utilization=75%, max_memory_utilization (weights)=75%, max_input_aligner_utilization=75%, max_apu_utilization=75% [info] Using Single-context flow [info] Resources optimization guidelines: Strategy -\u003e GREEDY Objective -\u003e MAX_FPS [info] Resources optimization params: max_control_utilization=75%, max_compute_utilization=75%, max_compute_16bit_utilization=75%, max_memory_utilization (weights)=75%, max_input_aligner_utilization=75%, max_apu_utilization=75% Validating context_0 layer by layer (100%) + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + ● Finished [info] Solving the allocation (Mapping), time per context: 59m 59s Context:0/0 Iteration 4: Trying parallel mapping... cluster_0 cluster_1 cluster_2 cluster_3 cluster_4 cluster_5 cluster_6 cluster_7 prepost worker0 V V V V V V V V V worker1 * * * * * * * * V worker2 * * * * * * * * V worker3 * * * * * * * * V 00:05 Reverts on cluster mapping: 0 Reverts on inter-cluster connectivity: 0 Reverts on pre-mapping validation: 0 Reverts on split failed: 0 [info] Iterations: 4 Reverts on cluster mapping: 0 Reverts on inter-cluster connectivity: 0 Reverts on pre-mapping validation: 0 Reverts on split failed: 0 [info] +-----------+---------------------+---------------------+--------------------+ [info] | Cluster | Control Utilization | Compute Utilization | Memory Utilization | [info] +-----------+---------------------+---------------------+--------------------+ [info] | cluster_0 | 68.8% | 40.6% | 15.6% | [info] | cluster_1 | 81.3% | 57.8% | 37.5% | [info] | cluster_2 | 81.3% | 96.9% | 93% | [info] | cluster_3 | 50% | 56.3% | 32% | [info] | cluster_4 | 62.5% | 45.3% | 29.7% | [info] | cluster_5 | 75% | 76.6% | 98.4% | [info] | cluster_6 | 81.3% | 56.3% | 37.5% | [info] | cluster_7 | 100% | 73.4% | 38.3% | [info] +-----------+---------------------+---------------------+--------------------+ [info] | Total | 75% | 62.9% | 47.8% | [info] +-----------+---------------------+---------------------+--------------------+ [info] Successful Mapping (allocation time: 23s) [info] Compiling context_0... [info] Bandwidth of model inputs: 9.375 Mbps, outputs: 9.22852 Mbps (for a single frame) [info] Bandwidth of DDR buffers: 0.0 Mbps (for a single frame) [info] Bandwidth of inter context tensors: 0.0 Mbps (for a single frame) [info] Building HEF... [info] Successful Compilation (compilation time: 14s) [info] Saved HAR to: /home/ubuntu/workspace/hailo/custom-train/data/model/yolov8n.har HEF file written to yolov8n.hef 4. 验证模型 4.1 使用 hailortcli 检测 在编译完成后，会生成 .hef 格式的模型文件，此时可以用 hailortcli 解析模型的信息\n4.1.1 安装 HailoRT 从 Developer Zone 下载并安装 HailoRT，通过 dpkg 安装\nsudo dpkg --install hailort_4.18.0_amd64.deb 4.1.2 检测模型信息 获取模型信息 hailortcli parse-hef ./yolov8n.hef 信息如下：\nArchitecture HEF was compiled for: HAILO8 Network group name: yolov8n, Single Context Network name: yolov8n/yolov8n VStream infos: Input yolov8n/input_layer1 UINT8, NHWC(640x640x3) Output yolov8n/yolov8_nms_postprocess FLOAT32, HAILO NMS(number of classes: 80, maximum bounding boxes per class: 100, maximum frame size: 160320) Operation: Op YOLOV8 Name: YOLOV8-Post-Process Score threshold: 0.200 IoU threshold: 0.70 Classes: 80 Cross classes: false Max bboxes per class: 100 Image height: 640 Image width: 640 运行模型 使用 hailortcli 运行模型，查看模型性能\nhailortcli run /home/ubuntu/workspace/project/hailo/model/yolov8n.hef --measure-temp 运行结果表明可以处理的图片帧率为 184.58\nRunning streaming inference (/home/ubuntu/workspace/project/hailo/model/yolov8n.hef): Transform data: true Type: auto Quantized: true Network yolov8n/yolov8n: 100% | 924 | FPS: 184.58 | ETA: 00:00:00 \u003e Inference result: Network group: yolov8n Frames count: 924 FPS: 184.59 Send Rate: 1814.58 Mbit/s Recv Rate: 1803.24 Mbit/s Device: 0000:04:00.0 Minimum chip temperature: 35.3433C Average chip temperature: 38.2565C Maximum chip temperature: 39.289C 4.2 使用 Python 进行对象检测 使用 Hailo 提供的 Demo，使用编译后的模型进行对象检测\n4.2.1 下载 Demo git clone https://github.com/hailo-ai/Hailo-Application-Code-Examples.git 4.2.2 运行对象检测 Demo 进入到对象检测 Demo 路径下\ncd Hailo-Application-Code-Examples/runtime/python/object_detection 运行 Demo\npython object_detection.py -n /home/ubuntu/workspace/project/hailo/model/yolov8n.hef -i ./zidane.jpg -l coco.txt 会提示将标注后到图片输出到指定的路径\n2024-09-08 17:42:27.549 | INFO | __main__:infer:175 - Inference was successful! Results have been saved in output_images 问题 FileNotFoundError: no alls found for requested hw_arch 这个错误是因为没有模型对应的 alls 文件，这个文件在 https://github.com/hailo-ai/hailo_model_zoo/ 仓库可以找到，路径是 hailo_model_zoo/cfg/alls；可以直接 clone 仓库到本地，然后将文件复制到虚拟环境中\n因为还依赖 postprocess_config目录下的 xxx_nms_config.json 文件，所以直接复制 hailo_model_zoo/cfg 目录\ngit clone https://github.com/hailo-ai/hailo_model_zoo.git cd hailo_model_zoo rm -rf /home/ubuntu/anaconda3/envs/hailo/lib/python3.10/site-packages/hailo_model_zoo/cfg cp -r hailo_model_zoo/hailo_model_zoo/cfg /home/ubuntu/anaconda3/envs/hailo/lib/python3.10/site-packages/hailo_model_zoo/cfg 安装 pygraphviz 失败，提示 fatal error: graphviz/cgraph.h: 没有那个文件或目录 改用 conda 安装: conda install pygraphviz 即可\ngcc -pthread -B /home/ubuntu/anaconda3/envs/hailo-zoo/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/ubuntu/anaconda3/envs/hailo-zoo/include -fPIC -O2 -isystem /home/ubuntu/anaconda3/envs/hailo-zoo/include -fPIC -DSWIG_PYTHON_STRICT_BYTE_CHAR -I/home/ubuntu/anaconda3/envs/hailo-zoo/include/python3.10 -c pygraphviz/graphviz_wrap.c -o build/temp.linux-x86_64-cpython-310/pygraphviz/graphviz_wrap.o pygraphviz/graphviz_wrap.c:9: warning: \"SWIG_PYTHON_STRICT_BYTE_CHAR\" redefined 9 | #define SWIG_PYTHON_STRICT_BYTE_CHAR | : note: this is the location of the previous definition pygraphviz/graphviz_wrap.c:3023:10: fatal error: graphviz/cgraph.h: 没有那个文件或目录 3023 | #include \"graphviz/cgraph.h\" | ^~~~~~~~~~~~~~~~~~~ compilation terminated. error: command '/usr/bin/gcc' failed with exit code 1 [end of output] note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for pygraphviz Failed to build pygraphviz ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pygraphviz) FileNotFoundError: Couldn’t find dataset in /home/ubuntu/.hailomz/data/models_files/coco/2021-06-18/coco_calib2017.tfrecord 通过 创建的 tfrecord 的路径可能和实际不一样，如要求的路径是 ~/.hailomz/data/models_files/coco/2021-06-18/coco_calib2017.tfrecord，实际的路径是 ~/.hailomz/data/models_files/coco/2023-08-03/coco_calib2017.tfrecord，通过指定 clibpath 参数即可:\nhailomz optimize --hw-arch hailo8l --har ./yolov8n.har yolov8n --calib-path ~/.hailomz/data/models_files/coco/2023-08-03/coco_calib2017.tfrecord ","wordCount":"1776","inLanguage":"en","datePublished":"2024-09-08T21:32:21+08:00","dateModified":"2024-09-08T21:32:21+08:00","author":{"@type":"Person","name":"HelloWood"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.hellowood.dev/posts/%E5%B0%86%E9%87%8D%E6%96%B0%E8%AE%AD%E7%BB%83%E7%9A%84-yolov8-%E6%A8%A1%E5%9E%8B%E7%BC%96%E8%AF%91%E4%B8%BA-hailo-8-%E5%92%8C-hailo-8l-%E6%94%AF%E6%8C%81%E7%9A%84%E6%A8%A1%E5%9E%8B/"},"publisher":{"@type":"Organization","name":"HelloWood","logo":{"@type":"ImageObject","url":"https://blog.hellowood.dev/favicon.ico"}}}</script><link rel=icon href=/images/avatar.png sizes=16x16><link rel=apple-touch-icon href=/images/avatar.png><link rel=manifest href=/images/avatar.png><link rel=stylesheet href=https://cdn.staticfile.org/lxgw-wenkai-webfont/1.6.0/style.css><link rel=stylesheet href=/css/main.min.7f82854fa0e999ec07c4835d3029de9f484030e254b80d470b3ca48eb934720e.css integrity="sha256-f4KFT6DpmewHxINdMCnen0hAMOJUuA1HCzykjrk0cg4=" crossorigin=anonymous media=screen><link rel=stylesheet href=/scss/highlight/github-dark.min.min.66034289ee9a113219a2c4aae0a8bd2095ab255c832a42efcf5863f10814e7a1.css><script src=/js/highlight.min.min.c607d6febd16934a82eb61d3a896ed9d869f54373cc63ce95864ed5488fe3128.js></script><script>hljs.highlightAll()</script><script>(()=>{var t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("theme");t&&e===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),t&&e==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),e==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script><meta name=google-adsense-account content="ca-pub-3401351766168985"><script type=text/javascript>(function(e,t,n,s,o,i,a){e[n]=e[n]||function(){(e[n].q=e[n].q||[]).push(arguments)},i=t.createElement(s),i.async=1,i.src="https://www.clarity.ms/tag/"+o,a=t.getElementsByTagName(s)[0],a.parentNode.insertBefore(i,a)})(window,document,"clarity","script","jtbhx98g62")</script><script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3401351766168985" crossorigin=anonymous></script><script defer src=https://analytics.us.umami.is/script.js data-website-id=73ff1c8c-9938-43cf-81af-e77e26b0cca3></script></head><body><main class=wrapper><nav class=navigation><section class=container><a class=navigation-brand data-umami-event=navigation-brand href=/>HOME
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><span></span><span></span><span></span></label><ul class=navigation-list id=navigation-list><li class="navigation-item navigation-menu"><a class=navigation-link data-umami-event=Blog href=/posts>Blog</a></li><li class="navigation-item navigation-menu"><a class=navigation-link data-umami-event=Tags href=/tags>Tags</a></li><li class="navigation-item navigation-menu"><a class=navigation-link data-umami-event=Archive href=/archives>Archive</a></li><li class="navigation-item navigation-menu"><a class=navigation-link data-umami-event=Dashboard href=https://umami.hellowood.dev/share/lab/Blog>Dashboard</a></li><li class="navigation-item menu-separator"><span>|</span></li><li class="navigation-item navigation-social"><a class=navigation-link data-umami-event=navigation-social href=https://github.com/helloworlde><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg></a></li><li class="navigation-item navigation-dark"><button id=mode type=button data-umami-event=toggle-theme aria-label="toggle user light or dark theme">
<span class=toggle-dark><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span>
<span class=toggle-light><svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button></li></ul></section></nav><div id=content><article class=blog-single><header class=blog-title><h1>将重新训练的 Yolov8 模型编译为 Hailo 8 和 Hailo 8L 支持的模型</h1></header><p><small>September 8, 2024&nbsp;· 1776 words&nbsp;· 9 min</small><p><div class=blog-toc><nav id=TableOfContents><ul><li><a href=#1-环境准备>1. 环境准备</a></li><li><a href=#2-训练-yolo-模型>2. 训练 yolo 模型</a><ul><li><a href=#21-安装依赖>2.1 安装依赖</a></li><li><a href=#22-训练模型>2.2 训练模型</a></li><li><a href=#23-将模型导出为-onnx-格式>2.3 将模型导出为 onnx 格式</a></li></ul></li><li><a href=#3-编译为-hailo-模型>3. 编译为 Hailo 模型</a><ul><li><a href=#31-准备环境>3.1 准备环境</a></li><li><a href=#32-编译模型>3.2 编译模型</a></li></ul></li><li><a href=#4-验证模型>4. 验证模型</a><ul><li><a href=#41-使用-hailortcli-检测>4.1 使用 hailortcli 检测</a></li><li><a href=#42-使用-python-进行对象检测>4.2 使用 Python 进行对象检测</a></li></ul></li><li><a href=#问题>问题</a></li></ul></nav></div><section class=blog-content><p>在 Ubuntu 22 将自行训练的 Yolov8n 模型编译为 Hailo8/Hailo8L 支持的 hef 格式的模型，在 Hailo8 上使用自行训练的模型进行对象检测</p><h2 id=1-环境准备>1. 环境准备</h2><ul><li>anaconda</li></ul><p>参考 <a href=https://docs.anaconda.com/anaconda/install/linux/>Installing on Linux</a> 安装</p><h2 id=2-训练-yolo-模型>2. 训练 yolo 模型</h2><h3 id=21-安装依赖>2.1 安装依赖</h3><ul><li>创建环境</li></ul><p>单独创建 ultralytics 的环境，用于训练和导出 yolo 模型为其他格式</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda create -n ultralytics python<span style=color:#f92672>=</span>3.10
</span></span></code></pre></div><ul><li>激活环境</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda activate ultralytics
</span></span></code></pre></div><ul><li>安装 ultralytics</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install ultralytics
</span></span></code></pre></div><p>等待安装完成后就可以使用 <code>yolo</code> 命令训练和导出模型了</p><h3 id=22-训练模型>2.2 训练模型</h3><p>训练模型可以参考 roboflow 的博客 <a href=https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset/>How to Train YOLOv8 Object Detection on a Custom Dataset</a>；为了测试验证流程，先使用官方提供的 yolov8n 当作训练的模型，可以从 <a href=https://github.com/ultralytics/assets/releases>https://github.com/ultralytics/assets/releases/</a> 下载，以 yolov8n 为例，下载地址为 <a href=https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt>https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt</a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>wget https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n.pt
</span></span></code></pre></div><h3 id=23-将模型导出为-onnx-格式>2.3 将模型导出为 onnx 格式</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>yolo export model<span style=color:#f92672>=</span>./yolov8n.pt imgsz<span style=color:#f92672>=</span><span style=color:#ae81ff>640</span> format<span style=color:#f92672>=</span>onnx opset<span style=color:#f92672>=</span><span style=color:#ae81ff>11</span>
</span></span></code></pre></div><p>imgsz=640 指定输入图片的尺寸（图像大小），即 640x640。这个尺寸是模型在推理时处理图像的分辨率</p><p>format=onnx 指定导出的模型格式为 ONNX（Open Neural Network Exchange）。ONNX 是一种支持在不同深度学习框架间转换和共享模型的开放格式</p><p>opset=11 指定 ONNX 的操作集版本为 11</p><h2 id=3-编译为-hailo-模型>3. 编译为 Hailo 模型</h2><h3 id=31-准备环境>3.1 准备环境</h3><h4 id=311-创建环境>3.1.1 创建环境</h4><ul><li>创建环境</li></ul><p>单独创建一个 hailo 的环境，避免和 ultralytics 依赖冲突</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda create -n hailo python<span style=color:#f92672>=</span>3.10
</span></span></code></pre></div><ul><li>激活环境</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda activate hailo
</span></span></code></pre></div><h4 id=312-安装-hailo_model_zoo>3.1.2 安装 hailo_model_zoo</h4><ul><li>安装 hailo_model_zoo</li></ul><p>从 <a href=https://hailo.ai/developer-zone/software-downloads/>Developer Zone</a> 下载并安装 hailo_model_zoo</p><p><img alt=homelab-tpu-hailo-software-download-hailo-model-zoo.png src=https://img.hellowood.dev/picture/homelab-tpu-hailo-software-download-hailo-model-zoo.png></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install hailo_model_zoo-2.12.0-py3-none-any.whl
</span></span></code></pre></div><h4 id=313-准备-coco-数据集>3.1.3 准备 Coco 数据集</h4><p>安装 Coco 数据集，用于评估、优化和编译模型；因为是通过 conda 安装的，代码所在路径是 conda env 路径 + 环境名称 + 包路径，即 <code>~/anaconda3/envs/hailo/lib/python3.10/site-packages/hailo_model_zoo</code></p><ul><li>准备 val 数据</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python ~/anaconda3/envs/hailo/lib/python3.10/site-packages/hailo_model_zoo/datasets/create_coco_tfrecord.py val2017
</span></span></code></pre></div><p>将会下载 coco 的 val 数据到 <code>~/.hailomz</code> 路径下；val 数据用于验证模型</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>2024-09-08 16:53:52.437311: I tensorflow/core/util/port.cc:110<span style=color:#f92672>]</span> oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable <span style=color:#e6db74>`</span>TF_ENABLE_ONEDNN_OPTS<span style=color:#f92672>=</span>0<span style=color:#e6db74>`</span>.
</span></span><span style=display:flex><span>2024-09-08 16:53:52.438390: I tensorflow/tsl/cuda/cudart_stub.cc:28<span style=color:#f92672>]</span> Could not find cuda drivers on your machine, GPU will not be used.
</span></span><span style=display:flex><span>2024-09-08 16:53:52.458864: I tensorflow/tsl/cuda/cudart_stub.cc:28<span style=color:#f92672>]</span> Could not find cuda drivers on your machine, GPU will not be used.
</span></span><span style=display:flex><span>2024-09-08 16:53:52.459101: I tensorflow/core/platform/cpu_feature_guard.cc:182<span style=color:#f92672>]</span> This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
</span></span><span style=display:flex><span>To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
</span></span><span style=display:flex><span>2024-09-08 16:53:52.747289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38<span style=color:#f92672>]</span> TF-TRT Warning: Could not find TensorRT
</span></span><span style=display:flex><span><span style=color:#ae81ff>48</span> / <span style=color:#ae81ff>5000</span> images have no annotations
</span></span><span style=display:flex><span>val2017 <span style=color:#75715e>#5000: /home/ubuntu/.hailomz/data/coco/val2017/000000365098.jpg: 100%|████████████████████████████████████████████████████████████████████████████████████████| 5000/5000 [00:09&lt;00:00, 526.82it/s]</span>
</span></span></code></pre></div><ul><li>准备 calib 数据</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python ~/anaconda3/envs/hailo/lib/python3.10/site-packages/hailo_model_zoo/datasets/create_coco_tfrecord.py calib2017
</span></span></code></pre></div><p>会下载名为 <code>train2017.zip</code> 大约 18G 的 Coco 数据集解压并处理，路径是 <code>～/.hailomz</code>；calib 数据集用于模型的量化校准</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>2024-09-08 16:54:19.758265: I tensorflow/core/util/port.cc:110<span style=color:#f92672>]</span> oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable <span style=color:#e6db74>`</span>TF_ENABLE_ONEDNN_OPTS<span style=color:#f92672>=</span>0<span style=color:#e6db74>`</span>.
</span></span><span style=display:flex><span>2024-09-08 16:54:19.759360: I tensorflow/tsl/cuda/cudart_stub.cc:28<span style=color:#f92672>]</span> Could not find cuda drivers on your machine, GPU will not be used.
</span></span><span style=display:flex><span>2024-09-08 16:54:19.779913: I tensorflow/tsl/cuda/cudart_stub.cc:28<span style=color:#f92672>]</span> Could not find cuda drivers on your machine, GPU will not be used.
</span></span><span style=display:flex><span>2024-09-08 16:54:19.780151: I tensorflow/core/platform/cpu_feature_guard.cc:182<span style=color:#f92672>]</span> This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
</span></span><span style=display:flex><span>To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
</span></span><span style=display:flex><span>2024-09-08 16:54:20.070489: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38<span style=color:#f92672>]</span> TF-TRT Warning: Could not find TensorRT
</span></span><span style=display:flex><span><span style=color:#ae81ff>1021</span> / <span style=color:#ae81ff>118287</span> images have no annotations
</span></span><span style=display:flex><span>calib2017 <span style=color:#75715e>#8192: /home/ubuntu/.hailomz/data/coco/train2017/000000171472.jpg: 100%|████████████████████████████████████████████████████████████████████████████████████| 8192/8192 [00:15&lt;00:00, 529.18it/s]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Done converting <span style=color:#ae81ff>8192</span> images
</span></span></code></pre></div><h4 id=314-安装-hailo_dataflow_compiler>3.1.4 安装 hailo_dataflow_compiler</h4><ul><li>安装 hailo_dataflow_compiler 依赖</li></ul><p>hailo_dataflow_compiler 用于将其他模型编译为 Hailo 支持的模型；在安装 hailo_dataflow_compiler 之前，需要先安装 pygraphviz，如果通过 hailo_dataflow_compiler 安装可能会提示 <code>fatal error: graphviz/cgraph.h: 没有那个文件或目录</code>，改成用 conda 安装即可</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda install pygraphviz
</span></span></code></pre></div><p>从 <a href=https://hailo.ai/developer-zone/software-downloads/>Developer Zone</a> 下载并安装 hailo_dataflow_compiler</p><p><img alt=homelab-tpu-hailo-software-download-hailo-data-compile.png src=https://img.hellowood.dev/picture/homelab-tpu-hailo-software-download-hailo-data-compile.png></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install hailo_dataflow_compiler-3.28.0-py3-none-linux_x86_64.whl
</span></span></code></pre></div><h3 id=32-编译模型>3.2 编译模型</h3><p>Hailo 不同硬件的模型不通用，所以需要在编译时通过 <code>--hw-arch</code> 指定硬件类型，以 Hailo8 为例，需要指定 <code>--hw-arch hailo8</code>；如果是 Hailo8L 则指定 <code>--hw-arch hailo8l</code></p><h4 id=321-解析模型>3.2.1 解析模型</h4><p>parse 用于将模型从各种框架解析为 HAR 格式（Hailo Archive）的步骤，HAR 是一个 tar.gz 存档文件，其中包含部署到 Hailo 运行时的图结构和权重的表示形式</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>hailomz parse --hw-arch hailo8 --ckpt ./best.onnx yolov8n
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>&lt;Hailo Model Zoo INFO&gt; Start run <span style=color:#66d9ef>for</span> network yolov8n ...
</span></span><span style=display:flex><span>&lt;Hailo Model Zoo INFO&gt; Initializing the runner...
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Translation started on ONNX model yolov8n
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Restored ONNX model yolov8n <span style=color:#f92672>(</span>completion time: 00:00:00.04<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Extracted ONNXRuntime meta-data <span style=color:#66d9ef>for</span> Hailo model <span style=color:#f92672>(</span>completion time: 00:00:00.26<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> NMS structure of yolov8 <span style=color:#f92672>(</span>or equivalent architecture<span style=color:#f92672>)</span> was detected.
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> In order to use HailoRT post-processing capabilities, these end node names should be used: /model.22/cv2.0/cv2.0.2/Conv /model.22/cv3.0/cv3.0.2/Conv /model.22/cv2.1/cv2.1.2/Conv /model.22/cv3.1/cv3.1.2/Conv /model.22/cv2.2/cv2.2.2/Conv /model.22/cv3.2/cv3.2.2/Conv.
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Start nodes mapped from original model: <span style=color:#e6db74>&#39;images&#39;</span>: <span style=color:#e6db74>&#39;yolov8n/input_layer1&#39;</span>.
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> End nodes mapped from original model: <span style=color:#e6db74>&#39;/model.22/cv2.0/cv2.0.2/Conv&#39;</span>, <span style=color:#e6db74>&#39;/model.22/cv3.0/cv3.0.2/Conv&#39;</span>, <span style=color:#e6db74>&#39;/model.22/cv2.1/cv2.1.2/Conv&#39;</span>, <span style=color:#e6db74>&#39;/model.22/cv3.1/cv3.1.2/Conv&#39;</span>, <span style=color:#e6db74>&#39;/model.22/cv2.2/cv2.2.2/Conv&#39;</span>, <span style=color:#e6db74>&#39;/model.22/cv3.2/cv3.2.2/Conv&#39;</span>.
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Translation completed on ONNX model yolov8n <span style=color:#f92672>(</span>completion time: 00:00:00.67<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Saved HAR to: /home/ubuntu/workspace/hailo/custom-train/data/model/yolov8n.har
</span></span></code></pre></div><h4 id=322-量化模型>3.2.2 量化模型</h4><p>optimize 输入是 Hailo 模型状态下的 HAR 文件（优化前；具有本机权重），输出将是具有量化权重的量化 HAR 文件</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>hailomz optimize --hw-arch hailo8 --har ./yolov8n.har yolov8n
</span></span></code></pre></div><p>输出如下；如果这个过程提示 <code>FileNotFoundError: no alls found for requested hw_arch</code>，参考后面问题部分处理</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>&lt;Hailo Model Zoo INFO&gt; Start run <span style=color:#66d9ef>for</span> network yolov8n ...
</span></span><span style=display:flex><span>&lt;Hailo Model Zoo INFO&gt; Initializing the hailo8 runner...
</span></span><span style=display:flex><span>&lt;Hailo Model Zoo INFO&gt; Preparing calibration data...
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Loading model script commands to yolov8n from /home/ubuntu/anaconda3/envs/hailo/lib/python3.10/site-packages/hailo_model_zoo/cfg/alls/hailo8/base/yolov8n.alls
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Starting Model Optimization
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>warning<span style=color:#f92672>]</span> Reducing optimization level to <span style=color:#ae81ff>0</span> <span style=color:#f92672>(</span>the accuracy won<span style=color:#e6db74>&#39;t be optimized and compression won&#39;</span>t be used<span style=color:#f92672>)</span> because there<span style=color:#960050;background-color:#1e0010>&#39;</span>s no available GPU
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>warning<span style=color:#f92672>]</span> Running model optimization with zero level of optimization is not recommended <span style=color:#66d9ef>for</span> production use and might lead to suboptimal accuracy results
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Model received quantization params from the hn
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Starting Mixed Precision
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Mixed Precision is <span style=color:#66d9ef>done</span> <span style=color:#f92672>(</span>completion time is 00:00:00.26<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Layer Norm Decomposition skipped
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Starting Stats Collector
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Using dataset with <span style=color:#ae81ff>64</span> entries <span style=color:#66d9ef>for</span> calibration
</span></span><span style=display:flex><span>Calibration: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 64/64 <span style=color:#f92672>[</span>00:15&lt;00:00,  4.25entries/s<span style=color:#f92672>]</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Stats Collector is <span style=color:#66d9ef>done</span> <span style=color:#f92672>(</span>completion time is 00:00:15.89<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Starting Fix zp_comp Encoding
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Fix zp_comp Encoding is <span style=color:#66d9ef>done</span> <span style=color:#f92672>(</span>completion time is 00:00:00.00<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> matmul_equalization skipped
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Finetune encoding skipped
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Bias Correction skipped
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Adaround skipped
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Fine Tune skipped
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Layer Noise Analysis skipped
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Model Optimization is <span style=color:#66d9ef>done</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Saved HAR to: /home/ubuntu/workspace/hailo/custom-train/data/model/yolov8n.har
</span></span></code></pre></div><h4 id=323-编译模型>3.2.3 编译模型</h4><p>compile 将模型转换为HEF可执行格式</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>hailomz compile  yolov8n --hw-arch hailo8 --har ./yolov8n.har
</span></span></code></pre></div><p>输出如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>&lt;Hailo Model Zoo INFO&gt; Start run <span style=color:#66d9ef>for</span> network yolov8n ...
</span></span><span style=display:flex><span>&lt;Hailo Model Zoo INFO&gt; Initializing the hailo8 runner...
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Loading model script commands to yolov8n from /home/ubuntu/anaconda3/envs/hailo/lib/python3.10/site-packages/hailo_model_zoo/cfg/alls/hailo8/base/yolov8n.alls
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> To achieve optimal performance, set the compiler_optimization_level to <span style=color:#e6db74>&#34;max&#34;</span> by adding performance_param<span style=color:#f92672>(</span>compiler_optimization_level<span style=color:#f92672>=</span>max<span style=color:#f92672>)</span> to the model script. Note that this may increase compilation time.
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Adding an output layer after conv41
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Adding an output layer after conv42
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Adding an output layer after conv52
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Adding an output layer after conv53
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Adding an output layer after conv62
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Adding an output layer after conv63
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Loading network parameters
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>warning<span style=color:#f92672>]</span> Output order different size
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Starting Hailo allocation and compilation flow
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Using Single-context flow
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Resources optimization guidelines: Strategy -&gt; GREEDY Objective -&gt; MAX_FPS
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Resources optimization params: max_control_utilization<span style=color:#f92672>=</span>75%, max_compute_utilization<span style=color:#f92672>=</span>75%, max_compute_16bit_utilization<span style=color:#f92672>=</span>75%, max_memory_utilization <span style=color:#f92672>(</span>weights<span style=color:#f92672>)=</span>75%, max_input_aligner_utilization<span style=color:#f92672>=</span>75%, max_apu_utilization<span style=color:#f92672>=</span>75%
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Using Single-context flow
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Resources optimization guidelines: Strategy -&gt; GREEDY Objective -&gt; MAX_FPS
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Resources optimization params: max_control_utilization<span style=color:#f92672>=</span>75%, max_compute_utilization<span style=color:#f92672>=</span>75%, max_compute_16bit_utilization<span style=color:#f92672>=</span>75%, max_memory_utilization <span style=color:#f92672>(</span>weights<span style=color:#f92672>)=</span>75%, max_input_aligner_utilization<span style=color:#f92672>=</span>75%, max_apu_utilization<span style=color:#f92672>=</span>75%
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Validating context_0 layer by layer <span style=color:#f92672>(</span>100%<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span> +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +
</span></span><span style=display:flex><span> +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +
</span></span><span style=display:flex><span> +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +
</span></span><span style=display:flex><span> +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +
</span></span><span style=display:flex><span> +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +  +
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>● Finished
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Solving the allocation <span style=color:#f92672>(</span>Mapping<span style=color:#f92672>)</span>, time per context: 59m 59s
</span></span><span style=display:flex><span>Context:0/0 Iteration 4: Trying parallel mapping...
</span></span><span style=display:flex><span>          cluster_0  cluster_1  cluster_2  cluster_3  cluster_4  cluster_5  cluster_6  cluster_7  prepost
</span></span><span style=display:flex><span> worker0  V          V          V          V          V          V          V          V          V
</span></span><span style=display:flex><span> worker1  *          *          *          *          *          *          *          *          V
</span></span><span style=display:flex><span> worker2  *          *          *          *          *          *          *          *          V
</span></span><span style=display:flex><span> worker3  *          *          *          *          *          *          *          *          V
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  00:05
</span></span><span style=display:flex><span>Reverts on cluster mapping: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>Reverts on inter-cluster connectivity: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>Reverts on pre-mapping validation: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>Reverts on split failed: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Iterations: <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>Reverts on cluster mapping: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>Reverts on inter-cluster connectivity: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>Reverts on pre-mapping validation: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>Reverts on split failed: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> +-----------+---------------------+---------------------+--------------------+
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> | Cluster   | Control Utilization | Compute Utilization | Memory Utilization |
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> +-----------+---------------------+---------------------+--------------------+
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> | cluster_0 | 68.8%               | 40.6%               | 15.6%              |
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> | cluster_1 | 81.3%               | 57.8%               | 37.5%              |
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> | cluster_2 | 81.3%               | 96.9%               | 93%                |
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> | cluster_3 | 50%                 | 56.3%               | 32%                |
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> | cluster_4 | 62.5%               | 45.3%               | 29.7%              |
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> | cluster_5 | 75%                 | 76.6%               | 98.4%              |
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> | cluster_6 | 81.3%               | 56.3%               | 37.5%              |
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> | cluster_7 | 100%                | 73.4%               | 38.3%              |
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> +-----------+---------------------+---------------------+--------------------+
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> | Total     | 75%                 | 62.9%               | 47.8%              |
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> +-----------+---------------------+---------------------+--------------------+
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Successful Mapping <span style=color:#f92672>(</span>allocation time: 23s<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Compiling context_0...
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Bandwidth of model inputs: 9.375 Mbps, outputs: 9.22852 Mbps <span style=color:#f92672>(</span><span style=color:#66d9ef>for</span> a single frame<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Bandwidth of DDR buffers: 0.0 Mbps <span style=color:#f92672>(</span><span style=color:#66d9ef>for</span> a single frame<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Bandwidth of inter context tensors: 0.0 Mbps <span style=color:#f92672>(</span><span style=color:#66d9ef>for</span> a single frame<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Building HEF...
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Successful Compilation <span style=color:#f92672>(</span>compilation time: 14s<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>info<span style=color:#f92672>]</span> Saved HAR to: /home/ubuntu/workspace/hailo/custom-train/data/model/yolov8n.har
</span></span><span style=display:flex><span>&lt;Hailo Model Zoo INFO&gt; HEF file written to yolov8n.hef
</span></span></code></pre></div><h2 id=4-验证模型>4. 验证模型</h2><h3 id=41-使用-hailortcli-检测>4.1 使用 hailortcli 检测</h3><p>在编译完成后，会生成 <code>.hef</code> 格式的模型文件，此时可以用 <code>hailortcli</code> 解析模型的信息</p><h4 id=411-安装-hailort>4.1.1 安装 HailoRT</h4><p>从 <a href=https://hailo.ai/developer-zone/software-downloads/>Developer Zone</a> 下载并安装 HailoRT，通过 dpkg 安装</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sudo dpkg --install hailort_4.18.0_amd64.deb
</span></span></code></pre></div><p><img alt=homelab-tpu-hailo-software-download-page.png src=https://img.hellowood.dev/picture/homelab-tpu-hailo-software-download-page.png></p><h4 id=412-检测模型信息>4.1.2 检测模型信息</h4><ul><li>获取模型信息</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>hailortcli parse-hef ./yolov8n.hef
</span></span></code></pre></div><p>信息如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>Architecture HEF was compiled <span style=color:#66d9ef>for</span>: HAILO8
</span></span><span style=display:flex><span>Network group name: yolov8n, Single Context
</span></span><span style=display:flex><span>    Network name: yolov8n/yolov8n
</span></span><span style=display:flex><span>        VStream infos:
</span></span><span style=display:flex><span>            Input  yolov8n/input_layer1 UINT8, NHWC<span style=color:#f92672>(</span>640x640x3<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>            Output yolov8n/yolov8_nms_postprocess FLOAT32, HAILO NMS<span style=color:#f92672>(</span>number of classes: 80, maximum bounding boxes per class: 100, maximum frame size: 160320<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>            Operation:
</span></span><span style=display:flex><span>                Op YOLOV8
</span></span><span style=display:flex><span>                Name: YOLOV8-Post-Process
</span></span><span style=display:flex><span>                Score threshold: 0.200
</span></span><span style=display:flex><span>                IoU threshold: 0.70
</span></span><span style=display:flex><span>                Classes: <span style=color:#ae81ff>80</span>
</span></span><span style=display:flex><span>                Cross classes: false
</span></span><span style=display:flex><span>                Max bboxes per class: <span style=color:#ae81ff>100</span>
</span></span><span style=display:flex><span>                Image height: <span style=color:#ae81ff>640</span>
</span></span><span style=display:flex><span>                Image width: <span style=color:#ae81ff>640</span>
</span></span></code></pre></div><ul><li>运行模型</li></ul><p>使用 hailortcli 运行模型，查看模型性能</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>hailortcli run /home/ubuntu/workspace/project/hailo/model/yolov8n.hef --measure-temp
</span></span></code></pre></div><p>运行结果表明可以处理的图片帧率为 184.58</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>Running streaming inference <span style=color:#f92672>(</span>/home/ubuntu/workspace/project/hailo/model/yolov8n.hef<span style=color:#f92672>)</span>:
</span></span><span style=display:flex><span>  Transform data: true
</span></span><span style=display:flex><span>    Type:      auto
</span></span><span style=display:flex><span>    Quantized: true
</span></span><span style=display:flex><span>Network yolov8n/yolov8n: 100% | <span style=color:#ae81ff>924</span> | FPS: 184.58 | ETA: 00:00:00
</span></span><span style=display:flex><span>&gt; Inference result:
</span></span><span style=display:flex><span> Network group: yolov8n
</span></span><span style=display:flex><span>    Frames count: <span style=color:#ae81ff>924</span>
</span></span><span style=display:flex><span>    FPS: 184.59
</span></span><span style=display:flex><span>    Send Rate: 1814.58 Mbit/s
</span></span><span style=display:flex><span>    Recv Rate: 1803.24 Mbit/s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  Device: 0000:04:00.0
</span></span><span style=display:flex><span>    Minimum chip temperature: 35.3433C
</span></span><span style=display:flex><span>    Average chip temperature: 38.2565C
</span></span><span style=display:flex><span>    Maximum chip temperature: 39.289C
</span></span></code></pre></div><h3 id=42-使用-python-进行对象检测>4.2 使用 Python 进行对象检测</h3><p>使用 Hailo 提供的 Demo，使用编译后的模型进行对象检测</p><h4 id=421-下载-demo>4.2.1 下载 Demo</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/hailo-ai/Hailo-Application-Code-Examples.git
</span></span></code></pre></div><h4 id=422-运行对象检测-demo>4.2.2 运行对象检测 Demo</h4><p>进入到对象检测 Demo 路径下</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cd Hailo-Application-Code-Examples/runtime/python/object_detection
</span></span></code></pre></div><p>运行 Demo</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python object_detection.py -n /home/ubuntu/workspace/project/hailo/model/yolov8n.hef -i ./zidane.jpg -l coco.txt
</span></span></code></pre></div><p>会提示将标注后到图片输出到指定的路径</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>2024-09-08 17:42:27.549 | INFO     | __main__:infer:175 - Inference was successful! Results have been saved in output_images
</span></span></code></pre></div><h2 id=问题>问题</h2><ol><li>FileNotFoundError: no alls found for requested hw_arch</li></ol><p>这个错误是因为没有模型对应的 alls 文件，这个文件在 <a href=https://github.com/hailo-ai/hailo_model_zoo/>https://github.com/hailo-ai/hailo_model_zoo/</a> 仓库可以找到，路径是 <code>hailo_model_zoo/cfg/alls</code>；可以直接 clone 仓库到本地，然后将文件复制到虚拟环境中</p><p>因为还依赖 <code>postprocess_config</code>目录下的 <code>xxx_nms_config.json</code> 文件，所以直接复制 <code>hailo_model_zoo/cfg</code> 目录</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/hailo-ai/hailo_model_zoo.git
</span></span><span style=display:flex><span>cd hailo_model_zoo
</span></span><span style=display:flex><span>rm -rf /home/ubuntu/anaconda3/envs/hailo/lib/python3.10/site-packages/hailo_model_zoo/cfg
</span></span><span style=display:flex><span>cp -r hailo_model_zoo/hailo_model_zoo/cfg /home/ubuntu/anaconda3/envs/hailo/lib/python3.10/site-packages/hailo_model_zoo/cfg
</span></span></code></pre></div><ol start=2><li>安装 pygraphviz 失败，提示 <code>fatal error: graphviz/cgraph.h: 没有那个文件或目录</code></li></ol><p>改用 conda 安装: <code>conda install pygraphviz</code> 即可</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>      gcc -pthread -B /home/ubuntu/anaconda3/envs/hailo-zoo/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/ubuntu/anaconda3/envs/hailo-zoo/include -fPIC -O2 -isystem /home/ubuntu/anaconda3/envs/hailo-zoo/include -fPIC -DSWIG_PYTHON_STRICT_BYTE_CHAR -I/home/ubuntu/anaconda3/envs/hailo-zoo/include/python3.10 -c pygraphviz/graphviz_wrap.c -o build/temp.linux-x86_64-cpython-310/pygraphviz/graphviz_wrap.o
</span></span><span style=display:flex><span>      pygraphviz/graphviz_wrap.c:9: warning: <span style=color:#e6db74>&#34;SWIG_PYTHON_STRICT_BYTE_CHAR&#34;</span> redefined
</span></span><span style=display:flex><span>          <span style=color:#ae81ff>9</span> | <span style=color:#75715e>#define SWIG_PYTHON_STRICT_BYTE_CHAR</span>
</span></span><span style=display:flex><span>            |
</span></span><span style=display:flex><span>      &lt;command-line&gt;: note: this is the location of the previous definition
</span></span><span style=display:flex><span>      pygraphviz/graphviz_wrap.c:3023:10: fatal error: graphviz/cgraph.h: 没有那个文件或目录
</span></span><span style=display:flex><span>       <span style=color:#ae81ff>3023</span> | <span style=color:#75715e>#include &#34;graphviz/cgraph.h&#34;</span>
</span></span><span style=display:flex><span>            |          ^~~~~~~~~~~~~~~~~~~
</span></span><span style=display:flex><span>      compilation terminated.
</span></span><span style=display:flex><span>      error: command <span style=color:#e6db74>&#39;/usr/bin/gcc&#39;</span> failed with exit code <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>[</span>end of output<span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  note: This error originates from a subprocess, and is likely not a problem with pip.
</span></span><span style=display:flex><span>  ERROR: Failed building wheel <span style=color:#66d9ef>for</span> pygraphviz
</span></span><span style=display:flex><span>Failed to build pygraphviz
</span></span><span style=display:flex><span>ERROR: ERROR: Failed to build installable wheels <span style=color:#66d9ef>for</span> some pyproject.toml based projects <span style=color:#f92672>(</span>pygraphviz<span style=color:#f92672>)</span>
</span></span></code></pre></div><ol start=3><li>FileNotFoundError: Couldn&rsquo;t find dataset in /home/ubuntu/.hailomz/data/models_files/coco/2021-06-18/coco_calib2017.tfrecord</li></ol><p>通过 创建的 tfrecord 的路径可能和实际不一样，如要求的路径是 <code>~/.hailomz/data/models_files/coco/2021-06-18/coco_calib2017.tfrecord</code>，实际的路径是 <code>~/.hailomz/data/models_files/coco/2023-08-03/coco_calib2017.tfrecord</code>，通过指定 <code>clibpath</code> 参数即可:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>hailomz optimize --hw-arch hailo8l --har ./yolov8n.har yolov8n --calib-path ~/.hailomz/data/models_files/coco/2023-08-03/coco_calib2017.tfrecord
</span></span></code></pre></div></section><div class=paginator><a class=next href=https://blog.hellowood.dev/posts/frigate-%E4%BD%BF%E7%94%A8-hailo-8-%E6%88%96-hailo-8l-%E8%BF%9B%E8%A1%8C%E5%AF%B9%E8%B1%A1%E8%AF%86%E5%88%AB/><span>Frigate 使用 Hailo 8 或 Hailo 8L 进行对象识别</span><svg class="icon" width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M3.77086 21.1546C11.0491 22.698 21.4339 21.7773 21.4339 16.3608V4.63375c0-.69413-.075800000000001-1.3284-.2422-1.86588M3.77086 21.1546C1.9934 20.7777.973585 18.7264 1.08749 16.688c.17931-3.209.06972-7.25665-.08236-10.47293C.87809 3.52811 3.12891 1.16316 5.51029 1.25008c4.25565.15534 9.86671-.04779 13.28091-.24466 1.2952-.074686 2.0494.62843 2.4005 1.76245M3.77086 21.1546C4.56586 21.4723 5.49168 21.7879 6.5 22.0658M21.1917 2.76787c1.918 1.4143 1.9383 9.65123 1.7087 13.59293-2.0526 7.6586-10.5943 7.3054-16.4004 5.705M21.1917 2.76787C21.7612 4.51192 22.7203 9.67216 22 16.3608 21.2797 23.0494 11.3665 22.9511 6.5 22.0658M12.055 9C12.711 9.61644 14.3679 10.997 15.9519 11.7966 16.0174 11.8297 16.0154 11.9753 15.9494 12.0063 14.945 12.4779 13.0706 13.9264 12.055 15m3.5006-3.0333C13.1345 12.0608 8 12 6 11" stroke="currentcolor" stroke-linecap="round"/></svg></a></div><div class=related-resources><h3>Related Resources</h3><nav><ul><li><a href=/posts/ubuntu22-%E5%AE%89%E8%A3%85%E5%88%9D%E5%A7%8B%E5%8C%96-hailo-8%E7%B3%BB%E5%88%97-tpu-%E5%8A%A0%E9%80%9F%E5%99%A8/>Ubuntu22 安装初始化 Hailo 8系列 TPU 加速器</a></li><li><a href=/posts/ubuntu-22-%E5%9C%A8-docker-%E5%AE%B9%E5%99%A8%E4%B8%AD%E4%BD%BF%E7%94%A8-nvidia-%E6%98%BE%E5%8D%A1/>Ubuntu 22 在 Docker 容器中使用 NVIDIA 显卡</a></li><li><a href=/posts/ubuntu-22-%E8%BF%90%E8%A1%8C-google-coral-tpu-%E7%A4%BA%E4%BE%8B/>Ubuntu 22 运行 Google Coral TPU 示例</a></li><li><a href=/posts/ubuntu-22-%E5%AE%89%E8%A3%85-google-coral-tpu-nvme-%E9%A9%B1%E5%8A%A8/>Ubuntu 22 安装 Google Coral TPU NVME 驱动</a></li><li><a href=/posts/ubuntu-22-%E5%AE%89%E8%A3%85-nvidia-%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8%E5%92%8C-cuda/>Ubuntu 22 安装 NVIDIA 显卡驱动和 CUDA</a></li><li><a href=/posts/ubuntu-22-%E5%AE%89%E8%A3%85-intel-n100-%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8/>Ubuntu 22 安装 Intel N100 显卡驱动</a></li><li><a href=/posts/ubuntu-22-%E7%8E%AF%E5%A2%83%E5%88%9D%E5%A7%8B%E5%8C%96/>Ubuntu 22 环境初始化</a></li><li><a href=/posts/%E4%BD%BF%E7%94%A8-ubuntu-%E6%90%AD%E5%BB%BA-nfs-%E6%9C%8D%E5%8A%A1%E5%99%A8/>使用 Ubuntu 搭建 NFS 服务器</a></li></ul></nav><h3>Related Resources</h3><nav><ul><li><a href=/posts/frigate-%E4%BD%BF%E7%94%A8-hailo-8-%E6%88%96-hailo-8l-%E8%BF%9B%E8%A1%8C%E5%AF%B9%E8%B1%A1%E8%AF%86%E5%88%AB/>Frigate 使用 Hailo 8 或 Hailo 8L 进行对象识别</a></li><li><a href=/posts/ubuntu22-%E5%AE%89%E8%A3%85%E5%88%9D%E5%A7%8B%E5%8C%96-hailo-8%E7%B3%BB%E5%88%97-tpu-%E5%8A%A0%E9%80%9F%E5%99%A8/>Ubuntu22 安装初始化 Hailo 8系列 TPU 加速器</a></li></ul></nav><h3>Related Resources</h3><nav><ul><li><a href=/posts/frigate-%E4%BD%BF%E7%94%A8-hailo-8-%E6%88%96-hailo-8l-%E8%BF%9B%E8%A1%8C%E5%AF%B9%E8%B1%A1%E8%AF%86%E5%88%AB/>Frigate 使用 Hailo 8 或 Hailo 8L 进行对象识别</a></li><li><a href=/posts/ubuntu22-%E5%AE%89%E8%A3%85%E5%88%9D%E5%A7%8B%E5%8C%96-hailo-8%E7%B3%BB%E5%88%97-tpu-%E5%8A%A0%E9%80%9F%E5%99%A8/>Ubuntu22 安装初始化 Hailo 8系列 TPU 加速器</a></li></ul></nav></div></article></div><footer class=footer><p>&copy; 2024 <a href=https://blog.hellowood.dev/>HelloWood</a>
Powered by
<a href=https://gohugo.io/ rel=noopener target=_blank data-umami-event=to-hugo>Hugo️️</a>
<a href=https://github.com/guangzhengli/hugo-theme-ladder rel=noopener target=_blank data-umami-event=to-ladder>Ladder</a>
️</p></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g data-umami-event=top-link><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M10.5376 22.7916C11.0152 22.7207 22.5795 21.1781 22.0978 10.4211 22.0536 9.43274 21.9303 8.53367 21.7387 7.71865M10.5376 22.7916C16.876 22.3728 20.0969 19.8899 21.5383 16.9142M10.5376 22.7916C9.7707 22.9055 8.97982 22.8964 8.19743 22.7725M21.7387 7.71865C21.4988 6.69828 21.1518 5.80967 20.7188 5.04257m1.0199 2.67608C22.6022 10.1105 23.0542 13.7848 21.5383 16.9142M20.7188 5.04257c-3.5504-6.28886-12.88753-4.410077-16.44303.0C2.88063 6.77451-.0433281 11.1668 1.38159 16.6571c.89322 3.4417 3.7911 5.6365 6.81584 6.1154M20.7188 5.04257c1.3509 1.89783 3.3111 6.34223 1.6353 10.37273M21.5383 16.9142C21.8737 16.4251 22.1428 15.9235 22.3541 15.4153M8.19743 22.7725C12.1971 23.4683 20.6281 22.971 22.3541 15.4153M14 10.945C13.3836 10.289 12.003 8.63215 11.2034 7.04814 11.1703 6.98257 11.0247 6.98456 10.9937 7.05061 10.5221 8.05496 9.07362 9.92941 8 10.945m3.0333-3.50056C10.9392 9.86549 11 15 12 17" stroke="currentcolor" stroke-linecap="round"/></svg>
</a><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const s=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="Copy";function n(){t.innerHTML="Copied",setTimeout(()=>{t.innerHTML="Copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),n();return}const s=document.createRange();s.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(s);try{document.execCommand("copy"),n()}catch{}o.removeRange(s)}),e.parentNode.appendChild(t)})</script></main></body><script src=https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin=anonymous referrerpolicy=no-referrer></script><script>const images=Array.from(document.querySelectorAll(".blog-content img"));images.forEach(e=>{mediumZoom(e,{margin:10,scrollOffset:40,container:null,template:null,background:"rgba(0, 0, 0, 0.5)"})})</script><script src=/main.min.6bb26b69159420159c74dc9e097b06a578ed2b68c701466a91a44a9632d851bd0af167a1b30012387b4c512b48ad9ad4d3394e04d77ae38d57e1920fe4ed34fe.js integrity="sha512-a7JraRWUIBWcdNyeCXsGpXjtK2jHAUZqkaRKljLYUb0K8WehswASOHtMUStIrZrU0zlOBNd6441X4ZIP5O00/g==" crossorigin=anonymous defer></script></html>